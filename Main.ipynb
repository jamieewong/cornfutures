{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamieewong/cornfutures/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ5t2oB9-KBt",
        "outputId": "ed94d06e-ee91-41ae-9d0c-2f08c309e728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-Ge-tGcy_etI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "qVHYUZ22pi1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess = transforms.Compose([\n",
        "  #  transforms.Resize((224, 224)),\n",
        " #   transforms.ToTensor(),\n",
        "\n",
        "#])\n",
        "\n",
        "#def load_image(path):\n",
        "    #img = Image.open(path).convert('RGB')\n",
        "   # return preprocess(img).unsqueeze(0)  # add batch dim\n",
        "\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converts to [C,H,W] tensor in [0,1]\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Standard ImageNet normalization\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def load_image(path):\n",
        "    img = Image.open(path).convert('RGB')  # Ensure 3-channel RGB\n",
        "    tensor = preprocess(img).unsqueeze(0)  # Add batch dimension [1,3,224,224]\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "gHJw0bObCXGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = models.resnet50(pretrained=True)\n",
        "resnet.fc = nn.Identity()  # Output vector will be size 2048\n",
        "\n",
        "resnet = resnet.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "resnet.eval()  # set to eval mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yABkH7NpCoGn",
        "outputId": "89efd4a3-0785-49a4-8ae1-3433a7565ab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 209MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Identity()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "img_path = '/content/drive/Shareddrives/MLSN-Team-12/MLSN-Team-12/Data/Satellite/area_0/area_0_2018-03-21.png'\n",
        "input_tensor = load_image(img_path).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    features = resnet(input_tensor)  # features shape: [1, 2048]\n",
        "\n",
        "print(features.shape)\n",
        "print(\"Feature vector:\", features.cpu().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVWkjgx2DE9z",
        "outputId": "394505d5-f5ce-441a-948a-09e54f1a9a9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2048])\n",
            "Feature vector: [[0.25888988 0.7675874  0.10958606 ... 0.07674737 0.00695726 0.303851  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = '/content/drive/Shareddrives/MLSN-Team-12/MLSN-Team-12/Data/Satellite-RGB'\n",
        "\n",
        "# assumes you already defined:\n",
        "# - device  (torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "# - resnet  (your ResNet encoder returning 2048-D vectors)\n",
        "# - load_image(path) → tensor shape (1,3,H,W)  (or (1,1,H,W) if NDVI)\n",
        "\n",
        "for root, dirs, files in os.walk(BASE_DIR):\n",
        "    # skip deleted folders\n",
        "    if 'deleted' in root.lower():\n",
        "        continue\n",
        "\n",
        "    rows = []\n",
        "    png_found = False\n",
        "\n",
        "    for fname in sorted(files):\n",
        "        if fname.lower().endswith('.png'):\n",
        "            png_found = True\n",
        "            img_path = os.path.join(root, fname)\n",
        "\n",
        "            # ----- 1️⃣ Extract date from name like \"area_2_2018-07-09.png\"\n",
        "            # split on underscores and take the last element, drop extension\n",
        "            date_str = os.path.splitext(fname)[0].split('_')[-1]\n",
        "\n",
        "            # ----- 2️⃣ Load image and compute features -----\n",
        "            input_tensor = load_image(img_path).to(device)\n",
        "            with torch.no_grad():\n",
        "                features = resnet(input_tensor)  # expect (1,2048)\n",
        "\n",
        "            # flatten to (2048,)\n",
        "            features = features.squeeze().cpu().numpy()\n",
        "            row = [date_str] + features.tolist()\n",
        "            rows.append(row)\n",
        "\n",
        "    # ----- 3️⃣ Write one CSV per folder -----\n",
        "    if png_found and len(rows) > 0:\n",
        "        csv_path = os.path.join(root, 'features.csv')\n",
        "        header = ['Date'] + [f'feat_{i:04d}' for i in range(len(rows[0]) - 1)]\n",
        "\n",
        "        with open(csv_path, 'w', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(header)\n",
        "            writer.writerows(rows)\n",
        "\n",
        "        print(f\"✅ Wrote {len(rows)} feature rows to {csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "awIN-CzZFvyh",
        "outputId": "1b26e053-4fd7-4543-ec4e-6fd48a6c6435",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'csv' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1995277166.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'csv' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = '/content/drive/Shareddrives/MLSN-Team-12/MLSN-Team-12/Data/Satellite-RGB'\n",
        "area_dirs = [os.path.join(BASE_DIR, d) for d in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, d)) and 'deleted' not in d.lower()]\n",
        "\n",
        "FUTURES_PATH = '/content/drive/Shareddrives/MLSN-Team-12/MLSN-Team-12/Data/Futures/futures_data.csv'\n",
        "\n",
        "fut_df = pd.read_csv(FUTURES_PATH)\n",
        "# make sure the column is called 'Date' and parse it\n",
        "fut_df['Date'] = pd.to_datetime(fut_df['Date'])\n",
        "# sort just in case\n",
        "fut_df = fut_df.sort_values('Date').reset_index(drop=True)\n",
        "# set index for fast lookup\n",
        "fut_df = fut_df.set_index('Date')\n",
        "\n",
        "fut_df.head()"
      ],
      "metadata": {
        "id": "3CI-a7zZomVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "BASE_DIR = '/content/drive/Shareddrives/MLSN-Team-12/MLSN-Team-12/Data/Satellite-RGB'\n",
        "\n",
        "# collect all CSVs recursively\n",
        "csv_paths = []\n",
        "for root, dirs, files in os.walk(BASE_DIR):\n",
        "    for f in files:\n",
        "        if f.lower() == \"features.csv\":\n",
        "            csv_paths.append(os.path.join(root, f))\n",
        "\n",
        "print(f\"Found {len(csv_paths)} feature CSVs\")\n",
        "\n",
        "# load and concatenate\n",
        "dfs = []\n",
        "for path in csv_paths:\n",
        "    area_name = os.path.basename(os.path.dirname(path))  # e.g. 'area_2'\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # ensure 'Date' is first column\n",
        "    if 'Date' not in df.columns:\n",
        "        continue\n",
        "    df = df[['Date'] + [c for c in df.columns if c != 'Date']]\n",
        "\n",
        "    # insert new column right after Date\n",
        "    df.insert(1, 'Area', area_name)\n",
        "\n",
        "    #add column 'Time Delta' to represent time (in days) between photos\n",
        "    df['Time Delta'] = df['Date'].diff()\n",
        "    df['Time Delta'] = df['Delta'].astype(int)\n",
        "    df['Time Delta'] = df['Time Delta']/(8.64*(10**13))\n",
        "    df['Time Delta'] = df['Time Delta'].astype(int)\n",
        "\n",
        "    dfs.append(df)\n",
        "\n",
        "# combine all areas into one DataFrame\n",
        "all_features_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "print(\"✅ Combined dataframe shape:\", all_features_df.shape)\n",
        "display(all_features_df.tail())\n"
      ],
      "metadata": {
        "id": "8Zf5WJaIp1dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Price change evaluation function"
      ],
      "metadata": {
        "id": "1CquNy2D-ZRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_future_price_change(fut_df, start_date, offset_days):\n",
        "\n",
        "    start_date = pd.to_datetime(start_date)\n",
        "\n",
        "    # 1. FIND FIRST DATE >= start_date\n",
        "    # Gets the closest available price datafram from the date\n",
        "    idx = fut_df.index.searchsorted(start_date, side=\"right\") - 1\n",
        "    if idx < 0:\n",
        "        return {\"No price data on or before starting date.\"}\n",
        "\n",
        "    # d1 is the dataframe of a selected date\n",
        "    d1 = fut_df.index[idx]\n",
        "    # p1 is pulling the close price for the date\n",
        "    p1 = fut_df.loc[d1, \"Close\"]\n",
        "\n",
        "    # 2. FIND FIRST DATE >= (start_date + offset)\n",
        "    target_end = start_date + pd.Timedelta(days=offset_days)\n",
        "    idx2 = fut_df.index.searchsorted(target_end)\n",
        "    if idx2 >= len(fut_df):\n",
        "        return {\"No price data on or after offset date.\"}\n",
        "\n",
        "    d2 = fut_df.index[idx2]\n",
        "    p2 = fut_df.loc[d2, \"Close\"]\n",
        "\n",
        "    # 3. price calculation\n",
        "    change = p2 - p1\n",
        "    pct_change = change / p1 * 100 if p1 != 0 else None\n",
        "\n",
        "    # output\n",
        "    return {\n",
        "        \"start_date_requested\": start_date.date(),\n",
        "        \"start_date_used\": d1.date(),\n",
        "        \"start_price\": float(p1),\n",
        "\n",
        "        \"end_date_requested\": target_end.date(),\n",
        "        \"end_date_used\": d2.date(),\n",
        "        \"end_price\": float(p2),\n",
        "\n",
        "        \"price_change\": float(change),\n",
        "        \"percent_change\": float(pct_change),\n",
        "    }"
      ],
      "metadata": {
        "id": "U4CnChOI-fTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OoDvMzxbF69P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing cells for price calculation function"
      ],
      "metadata": {
        "id": "l0KSn4dX-hNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example where start date does not have price, using closest before date\n",
        "get_future_price_change(fut_df, \"2017-07-4\", 8)"
      ],
      "metadata": {
        "id": "Nn3eIJS_-mEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example where start date and offset date have an available price\n",
        "get_future_price_change(fut_df, \"2017-07-3\", 8)"
      ],
      "metadata": {
        "id": "a3ZcaM7Z-nt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example where off set date does not have price, using closest price after offset date\n",
        "get_future_price_change(fut_df, \"2017-07-3\", 12)"
      ],
      "metadata": {
        "id": "QaOvyu1U-q_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FUTURES_PATH = '/content/drive/Shareddrives/MLSN-Team-12/MLSN-Team-12/Data/Futures/futures_data.csv'\n",
        "fut_df = pd.read_csv(FUTURES_PATH)\n",
        "fut_df['Date'] = pd.to_datetime(fut_df['Date'])\n",
        "fut_df = fut_df.sort_values('Date').set_index('Date')\n",
        "\n",
        "def price_on_or_after(date, df=fut_df, col='Close', max_lookahead=5):\n",
        "    if date in df.index:\n",
        "        return float(df.loc[date, col])\n",
        "    for i in range(1, max_lookahead+1):\n",
        "        d = date + pd.Timedelta(days=i)\n",
        "        if d in df.index:\n",
        "            return float(df.loc[d, col])\n",
        "    return None"
      ],
      "metadata": {
        "id": "oMci-2FTLrCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = '/content/drive/Shareddrives/MLSN-Team-12/MLSN-Team-12/Data/Satellite-RGB'\n",
        "\n",
        "# discover area folders (skip 'deleted')\n",
        "area_dirs = [os.path.join(BASE_DIR, d) for d in os.listdir(BASE_DIR)\n",
        "             if os.path.isdir(os.path.join(BASE_DIR, d)) and 'deleted' not in d.lower()]\n",
        "\n",
        "region2id = {os.path.basename(p): i for i, p in enumerate(sorted(area_dirs))}\n",
        "print(\"Areas:\", len(region2id), region2id)\n",
        "\n",
        "def load_area_features(area_path):\n",
        "    df = pd.read_csv(os.path.join(area_path, 'features.csv'))\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df = df.sort_values('Date').reset_index(drop=True)\n",
        "    feat_cols = [c for c in df.columns if c.startswith('feat_')]\n",
        "    return df[['Date'] + feat_cols], feat_cols\n",
        "\n",
        "def build_samples(area_dirs, window_size=6, horizon_days=14):\n",
        "    samples = []\n",
        "    for area_path in area_dirs:\n",
        "        area_name = os.path.basename(area_path)\n",
        "        csv_path = os.path.join(area_path, 'features.csv')\n",
        "        if not os.path.exists(csv_path):\n",
        "            continue\n",
        "\n",
        "        df, feat_cols = load_area_features(area_path)\n",
        "        if len(df) < window_size:\n",
        "            continue\n",
        "\n",
        "        for i in range(len(df) - window_size + 1):\n",
        "            window = df.iloc[i:i+window_size]\n",
        "            dates = window['Date'].to_numpy()\n",
        "            feats = window[feat_cols].to_numpy(dtype=np.float32)  # (T, 2048)\n",
        "\n",
        "            last_date = dates[-1]\n",
        "            price_last = price_on_or_after(last_date)                        # Close(t_last)\n",
        "            price_future = price_on_or_after(last_date + pd.Timedelta(days=horizon_days))  # Close(t_last+H)\n",
        "            if price_last is None or price_future is None:\n",
        "                continue\n",
        "            price_last = price_on_or_after(last_date)\n",
        "\n",
        "            target_diff = price_future - price_last                          # ΔPrice\n",
        "\n",
        "            # days since previous image per step\n",
        "            day_ints = dates.astype('datetime64[D]').astype(int)\n",
        "            dts = np.diff(day_ints)\n",
        "            dts = np.insert(dts, 0, 0).astype(np.float32)                    # (T,)\n",
        "\n",
        "            samples.append({\n",
        "    'feats': feats,            # (T, 2048)\n",
        "    'dts': dts,                # (T,)\n",
        "    'region_id': region2id[area_name],\n",
        "    'target_diff': float(target_diff),\n",
        "    'last_date': last_date,\n",
        "    'area': area_name,\n",
        "    'price_last': float(price_last),\n",
        "})\n",
        "    return samples\n",
        "\n",
        "# Build your training examples\n",
        "samples = build_samples(area_dirs, window_size=4, horizon_days=45)\n",
        "print(\"Total samples:\", len(samples))"
      ],
      "metadata": {
        "id": "DpRXvWR-LsKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDataset(Dataset):\n",
        "    def __init__(self, samples, normalize_target=True):\n",
        "        self.samples = samples\n",
        "        self.normalize_target = normalize_target\n",
        "        self.mean = np.mean([s['target_diff'] for s in samples]) if normalize_target else 0.0\n",
        "        self.std  = np.std([s['target_diff'] for s in samples]) + 1e-6 if normalize_target else 1.0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      s = self.samples[idx]\n",
        "      feats = torch.tensor(s['feats'], dtype=torch.float32)      # (T, 2048)\n",
        "      dts   = torch.tensor(s['dts'], dtype=torch.float32)        # (T,)\n",
        "      rid   = torch.tensor(s['region_id'], dtype=torch.long)\n",
        "\n",
        "    # normalize target diff\n",
        "      y = torch.tensor((s['target_diff'] - self.mean)/self.std, dtype=torch.float32)\n",
        "\n",
        "      price_last = torch.tensor(s['price_last'], dtype=torch.float32)  # scalar\n",
        "\n",
        "      last_date_int = np.datetime64(s['last_date']).astype('datetime64[D]').astype(int)\n",
        "\n",
        "    # return price_last as extra feature\n",
        "      return feats, dts, rid, y, last_date_int, price_last\n",
        "\n"
      ],
      "metadata": {
        "id": "Px06bcCjMgri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = LSTMDataset(samples, normalize_target=True)\n",
        "\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_set, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "ZrV9Nt38Mh24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ============================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================\n",
        "BASE_DIR = '/content/drive/MyDrive/MLSN-Team-12/Data/Satellite-RGB'\n",
        "FUTURES_PATH = '/content/drive/MyDrive/MLSN-Team-12/Data/Futures/futures_data.csv'\n",
        "WEATHER_DIR = '/content/drive/MyDrive/MLSN-Team-12/Data/Weather_Base'\n",
        "SAVE_DIR = '/content'\n",
        "\n",
        "SEQ_LEN = 21\n",
        "PREDICTION_OFFSET_DAYS = 14\n",
        "\n",
        "# Tuning (Config C - The Winner)\n",
        "LSTM_HIDDEN = 64\n",
        "LSTM_LAYERS = 2\n",
        "DROPOUT = 0.4\n",
        "LR = 1e-4\n",
        "WEIGHT_DECAY = 1e-3\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 60\n",
        "PATIENCE = 15\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ============================================================\n",
        "# 1. FEATURE ENGINEERING (ADDED SEASONALITY)\n",
        "# ============================================================\n",
        "def add_seasonality(df, date_col='Date'):\n",
        "    # IMPROVEMENT 2: Cyclic Seasonality\n",
        "    # Encodes \"Day of Year\" as coordinates on a circle.\n",
        "    # Close dates (Dec 31 and Jan 1) become close in value.\n",
        "    day = df[date_col].dt.dayofyear\n",
        "    df['Day_Sin'] = np.sin(2 * np.pi * day / 365.0)\n",
        "    df['Day_Cos'] = np.cos(2 * np.pi * day / 365.0)\n",
        "    return df\n",
        "\n",
        "def load_and_engineer_futures(path):\n",
        "    fut_df = pd.read_csv(path)\n",
        "    fut_df['Date'] = pd.to_datetime(fut_df['Date'])\n",
        "    fut_df = fut_df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "    # Add Seasonality\n",
        "    fut_df = add_seasonality(fut_df)\n",
        "\n",
        "    if 'Close Percent Change' in fut_df.columns:\n",
        "        fut_df.rename(columns={'Close Percent Change': 'Close_pct_change'}, inplace=True)\n",
        "    if 'Close_pct_change' not in fut_df.columns:\n",
        "        fut_df['Close_pct_change'] = fut_df['Close'].pct_change() * 100.0\n",
        "\n",
        "    if 'Volume' in fut_df.columns:\n",
        "        fut_df['Volume_log'] = np.log1p(fut_df['Volume'])\n",
        "    else:\n",
        "        fut_df['Volume_log'] = 0.0\n",
        "\n",
        "    delta = fut_df['Close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "    rs = gain / loss\n",
        "    fut_df['RSI'] = 100 - (100 / (1 + rs))\n",
        "    fut_df['RSI'] = fut_df['RSI'].fillna(50)\n",
        "\n",
        "    ema12 = fut_df['Close'].ewm(span=12, adjust=False).mean()\n",
        "    ema26 = fut_df['Close'].ewm(span=26, adjust=False).mean()\n",
        "    fut_df['MACD'] = ema12 - ema26\n",
        "\n",
        "    sma20 = fut_df['Close'].rolling(window=20).mean()\n",
        "    std20 = fut_df['Close'].rolling(window=20).std()\n",
        "    bb_upper = sma20 + (std20 * 2)\n",
        "    bb_lower = sma20 - (std20 * 2)\n",
        "    fut_df['BB_PctB'] = (fut_df['Close'] - bb_lower) / (bb_upper - bb_lower)\n",
        "    fut_df['BB_PctB'] = fut_df['BB_PctB'].fillna(0.5)\n",
        "\n",
        "    return fut_df.dropna().reset_index(drop=True)\n",
        "\n",
        "def load_and_engineer_weather(weather_dir):\n",
        "    print(f\"Scanning {weather_dir} for weather data...\")\n",
        "    weather_frames = []\n",
        "    for f in os.listdir(weather_dir):\n",
        "        if f.endswith('.csv'):\n",
        "            import re\n",
        "            match = re.search(r'(\\d+)', f)\n",
        "            if match:\n",
        "                area_name = f\"area_{match.group(1)}\"\n",
        "                path = os.path.join(weather_dir, f)\n",
        "                try:\n",
        "                    df = pd.read_csv(path)\n",
        "                    date_col = next((c for c in df.columns if 'date' in c.lower()), None)\n",
        "                    if not date_col: continue\n",
        "                    df = df.rename(columns={date_col: 'Date'})\n",
        "                    df['Date'] = pd.to_datetime(df['Date'])\n",
        "                    df = df.sort_values('Date')\n",
        "\n",
        "                    t_mean = (df['temp_max_C'] + df['temp_min_C']) / 2\n",
        "                    df['GDD'] = (t_mean - 10).clip(lower=0)\n",
        "                    df['GDD_Cum_7d'] = df['GDD'].rolling(7).sum()\n",
        "                    df['Precip_Sum_7d'] = df['precip_mm'].rolling(7).sum()\n",
        "\n",
        "                    cols_to_keep = ['Date', 'GDD_Cum_7d', 'Precip_Sum_7d']\n",
        "                    df = df[cols_to_keep].copy()\n",
        "                    df['Area'] = area_name\n",
        "                    weather_frames.append(df)\n",
        "                except Exception: continue\n",
        "\n",
        "    if not weather_frames: raise ValueError(\"No weather data found!\")\n",
        "    all_weather = pd.concat(weather_frames, ignore_index=True)\n",
        "\n",
        "    cols_to_fill = [c for c in all_weather.columns if c not in ['Area', 'Date']]\n",
        "    all_weather[cols_to_fill] = all_weather.groupby('Area')[cols_to_fill].transform(lambda x: x.ffill().bfill())\n",
        "    return all_weather\n",
        "\n",
        "def load_satellite_data(base_dir):\n",
        "    print(f\"Scanning {base_dir} for features.csv...\")\n",
        "    csv_paths = []\n",
        "    for root, dirs, files in os.walk(base_dir):\n",
        "        for f in files:\n",
        "            if f.lower() == \"features.csv\":\n",
        "                csv_paths.append(os.path.join(root, f))\n",
        "\n",
        "    dfs = []\n",
        "    for path in csv_paths:\n",
        "        area_name = os.path.basename(os.path.dirname(path))\n",
        "        try:\n",
        "            df = pd.read_csv(path)\n",
        "            if 'Date' in df.columns:\n",
        "                df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "                df = df.dropna(subset=['Date'])\n",
        "                df = df[['Date'] + [c for c in df.columns if c != 'Date']]\n",
        "                df.insert(1, 'Area', area_name)\n",
        "                df = df.sort_values('Date')\n",
        "                df['Time Delta'] = df['Date'].diff().dt.total_seconds().div(86400).fillna(0).astype(int)\n",
        "                dfs.append(df)\n",
        "        except Exception: continue\n",
        "\n",
        "    all_features_df = pd.concat(dfs, ignore_index=True)\n",
        "    all_features_df = all_features_df.sort_values('Date').reset_index(drop=True)\n",
        "    feat_cols = [c for c in all_features_df.columns if c.startswith('feat_')]\n",
        "    agg_dict = {c: 'mean' for c in feat_cols}\n",
        "    all_features_df = all_features_df.groupby(['Area', 'Date', 'Time Delta'], as_index=False).agg(agg_dict)\n",
        "    return all_features_df\n",
        "\n",
        "# ============================================================\n",
        "# 2. DATASET & MODEL\n",
        "# ============================================================\n",
        "class CornSequenceDataset(Dataset):\n",
        "    def __init__(self, features_df, fut_price_df, img_cols, fut_cols,\n",
        "                 seq_len=SEQ_LEN, prediction_offset=PREDICTION_OFFSET_DAYS):\n",
        "\n",
        "        self.features_df = features_df\n",
        "        self.fut_price_df = fut_price_df\n",
        "        self.img_cols = img_cols\n",
        "        self.fut_cols = fut_cols\n",
        "        self.seq_len = seq_len\n",
        "        self.prediction_offset = prediction_offset\n",
        "        self.area_to_id = {a: i for i, a in enumerate(sorted(features_df['Area'].unique()))}\n",
        "        self.area_groups = {area: group for area, group in features_df.groupby('Area')}\n",
        "\n",
        "        self.label_mean = 0.0\n",
        "        self.label_std = 1.0\n",
        "        self.label_transform = None\n",
        "        self.feature_transform = None\n",
        "\n",
        "        self.sequences = []\n",
        "        self._build_sequences()\n",
        "\n",
        "    def _build_sequences(self):\n",
        "        print(f\"Building sequences (L={self.seq_len})...\")\n",
        "        for area in self.area_groups:\n",
        "            area_data = self.area_groups[area].sort_values('Date').reset_index(drop=True)\n",
        "            n = len(area_data)\n",
        "            for i in range(0, n - self.seq_len + 1):\n",
        "                seq_end_idx = i + self.seq_len\n",
        "                last_img_date = area_data.iloc[seq_end_idx - 1]['Date']\n",
        "                target_ideal_date = last_img_date + pd.Timedelta(days=self.prediction_offset)\n",
        "                try:\n",
        "                    loc = self.fut_price_df.index.get_indexer([target_ideal_date], method='nearest')[0]\n",
        "                    if loc == -1: continue\n",
        "                    actual_target_date = self.fut_price_df.index[loc]\n",
        "                    if abs((actual_target_date - target_ideal_date).days) > 5: continue\n",
        "                    if last_img_date in self.fut_price_df.index:\n",
        "                        self.sequences.append({\n",
        "                            'area': area,\n",
        "                            'data_indices': (i, seq_end_idx),\n",
        "                            'base_date': last_img_date,\n",
        "                            'target_date': actual_target_date\n",
        "                        })\n",
        "                except: continue\n",
        "        self.sequences.sort(key=lambda x: x['target_date'])\n",
        "\n",
        "    def _compute_label(self, idx):\n",
        "        seq = self.sequences[idx]\n",
        "        p_base = float(self.fut_price_df.loc[seq['base_date'], 'Close'])\n",
        "        p_target = float(self.fut_price_df.loc[seq['target_date'], 'Close'])\n",
        "        raw_pct = (p_target - p_base) / p_base * 100.0\n",
        "        if self.label_transform: return self.label_transform(raw_pct)\n",
        "        return raw_pct\n",
        "\n",
        "    def __len__(self): return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq_info = self.sequences[idx]\n",
        "        start, end = seq_info['data_indices']\n",
        "        subset = self.area_groups[seq_info['area']].iloc[start:end]\n",
        "        img = torch.FloatTensor(subset[self.img_cols].values)\n",
        "        fut_raw = subset[self.fut_cols].values\n",
        "        if self.feature_transform: fut_raw = self.feature_transform(fut_raw)\n",
        "        fut = torch.FloatTensor(fut_raw)\n",
        "        aid = torch.LongTensor([self.area_to_id[seq_info['area']]] * self.seq_len)\n",
        "        time = torch.FloatTensor(subset['Time Delta'].values.reshape(-1, 1))\n",
        "        lbl = torch.FloatTensor([self._compute_label(idx)])\n",
        "        return img, fut, aid, time, lbl\n",
        "\n",
        "class CornETFModelFinal(nn.Module):\n",
        "    def __init__(self, img_dim, fut_dim, num_areas):\n",
        "        super().__init__()\n",
        "        self.img_bottleneck = nn.Sequential(\n",
        "            nn.Linear(img_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        self.fut_norm = nn.LayerNorm(fut_dim)\n",
        "        self.area_embed = nn.Embedding(num_areas, 16)\n",
        "        self.time_embed = nn.Linear(1, 8)\n",
        "\n",
        "        self.lstm_input_dim = 64 + fut_dim + 16 + 8\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.lstm_input_dim,\n",
        "            hidden_size=LSTM_HIDDEN,\n",
        "            num_layers=LSTM_LAYERS,\n",
        "            batch_first=True,\n",
        "            dropout=DROPOUT\n",
        "        )\n",
        "        self.dropout = nn.Dropout(DROPOUT)\n",
        "        self.head = nn.Linear(LSTM_HIDDEN, 1)\n",
        "\n",
        "    def forward(self, img, fut, aid, time):\n",
        "        img_c = self.img_bottleneck(img)\n",
        "        fut_c = self.fut_norm(fut)\n",
        "        area_v = self.area_embed(aid)\n",
        "        time_v = self.time_embed(time)\n",
        "        x = torch.cat([img_c, fut_c, area_v, time_v], dim=2)\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.head(self.dropout(out[:, -1, :]))\n",
        "\n",
        "# ============================================================\n",
        "# 3. EXECUTION\n",
        "# ============================================================\n",
        "def collate_fn(batch):\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    if not batch: return None\n",
        "    return (torch.stack([b[0] for b in batch]), torch.stack([b[1] for b in batch]),\n",
        "            torch.stack([b[2] for b in batch]), torch.stack([b[3] for b in batch]),\n",
        "            torch.stack([b[4] for b in batch]))\n",
        "\n",
        "def compute_normalization(dataset, train_idx):\n",
        "    labels = [dataset._compute_label(i) for i in train_idx]\n",
        "    labels = np.array(labels)\n",
        "    mean, std = float(labels.mean()), float(labels.std())\n",
        "    dataset.label_mean = mean\n",
        "    dataset.label_std = max(std, 2.0)\n",
        "    dataset.label_transform = lambda x: (x - mean) / (dataset.label_std + 1e-8)\n",
        "    print(f\"✅ Label Norm: Mean={mean:.4f}, Std={dataset.label_std:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(f\"Using Device: {DEVICE}\")\n",
        "    fut_df = load_and_engineer_futures(FUTURES_PATH)\n",
        "    fut_price_df = fut_df.set_index('Date')[['Close']].sort_index()\n",
        "    sat_df = load_satellite_data(BASE_DIR)\n",
        "    weather_df = load_and_engineer_weather(WEATHER_DIR)\n",
        "\n",
        "    sat_weather_df = pd.merge(sat_df, weather_df, on=['Area', 'Date'], how='left')\n",
        "    cols_to_fill = [c for c in sat_weather_df.columns if c not in ['Area', 'Date']]\n",
        "    sat_weather_df[cols_to_fill] = sat_weather_df.groupby('Area')[cols_to_fill].transform(lambda x: x.ffill().bfill())\n",
        "    merged_df = pd.merge(sat_weather_df, fut_df, on='Date', how='inner', suffixes=('', '_fut'))\n",
        "\n",
        "    img_cols = [c for c in merged_df.columns if c.startswith('feat_')]\n",
        "    fut_cols = [c for c in merged_df.columns if c not in ['Date', 'Area', 'Time Delta'] and not c.startswith('feat_')]\n",
        "\n",
        "    dataset = CornSequenceDataset(merged_df, fut_price_df, img_cols, fut_cols, seq_len=SEQ_LEN)\n",
        "    all_dates = pd.to_datetime([seq['target_date'] for seq in dataset.sequences])\n",
        "\n",
        "    folds = [\n",
        "        {'name': 'Fold 1: The Rally (Test 2021)', 'train_end': '2021-01-01', 'test_end': '2022-01-01'},\n",
        "        {'name': 'Fold 2: The Peak (Test 2022)',  'train_end': '2022-01-01', 'test_end': '2023-01-01'},\n",
        "        {'name': 'Fold 3: The Crash (Test 2023)', 'train_end': '2023-01-01', 'test_end': '2024-01-01'}\n",
        "    ]\n",
        "\n",
        "    final_results = []\n",
        "\n",
        "    for fold in folds:\n",
        "        print(f\"\\n>>> RUNNING: {fold['name']}\")\n",
        "\n",
        "        train_mask = all_dates < pd.Timestamp(fold['train_end'])\n",
        "        test_mask = (all_dates >= pd.Timestamp(fold['train_end'])) & (all_dates < pd.Timestamp(fold['test_end']))\n",
        "        train_idx = np.where(train_mask)[0]\n",
        "        val_idx = np.where(test_mask)[0]\n",
        "\n",
        "        if len(val_idx) < 50: continue\n",
        "\n",
        "        compute_normalization(dataset, train_idx)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        train_features = []\n",
        "        for i in train_idx[::10]:\n",
        "            seq_info = dataset.sequences[i]\n",
        "            start, end = seq_info['data_indices']\n",
        "            subset = dataset.area_groups[seq_info['area']].iloc[start:end]\n",
        "            train_features.append(subset[dataset.fut_cols].values)\n",
        "        scaler.fit(np.vstack(train_features))\n",
        "        dataset.feature_transform = lambda x: scaler.transform(x)\n",
        "\n",
        "        train_loader = DataLoader(Subset(dataset, train_idx), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "        val_loader = DataLoader(Subset(dataset, val_idx), batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "        model = CornETFModelFinal(\n",
        "            img_dim=len(img_cols), fut_dim=len(fut_cols), num_areas=len(dataset.area_to_id)\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "        # IMPROVEMENT 1: Huber Loss (Robust Regression)\n",
        "        # Acts like MSE for small errors, but linear for outliers.\n",
        "        # Prevents explosion when the model misses a crash.\n",
        "        criterion = nn.HuberLoss(delta=1.0)\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "        best_val = float('inf')\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "            model.train()\n",
        "            t_loss = 0\n",
        "            for img, fut, aid, time, lbl in train_loader:\n",
        "                img, fut, aid, time, lbl = img.to(DEVICE), fut.to(DEVICE), aid.to(DEVICE), time.to(DEVICE), lbl.to(DEVICE)\n",
        "                preds = model(img, fut, aid, time)\n",
        "                loss = criterion(preds, lbl)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                t_loss += loss.item()\n",
        "\n",
        "             model.eval()\n",
        "            v_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for img, fut, aid, time, lbl in val_loader:\n",
        "                    img, fut, aid, time, lbl = img.to(DEVICE), fut.to(DEVICE), aid.to(DEVICE), time.to(DEVICE), lbl.to(DEVICE)\n",
        "                    v_loss += criterion(model(img, fut, aid, time), lbl).item()\n",
        "\n",
        "            avg_v = v_loss/len(val_loader)\n",
        "            scheduler.step(avg_v)\n",
        "\n",
        "            if avg_v < best_val:\n",
        "                best_val = avg_v\n",
        "                patience_counter = 0\n",
        "                torch.save(model.state_dict(), f\"{SAVE_DIR}/best_model_{fold['name'].split(':')[0].replace(' ', '_')}.pt\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if (epoch+1) % 5 == 0:\n",
        "                print(f\"Fold Epoch {epoch+1}: Train={t_loss/len(train_loader):.4f} | Val={avg_v:.4f}\")\n",
        "\n",
        "            if patience_counter >= PATIENCE: break\n",
        "\n",
        "        results_table.append({'Fold': fold['name'], 'Best_MSE': best_val})\n",
        "\n",
        "    print(\"\\n📊 FINAL REPORT (Lowest Loss Wins)\")\n",
        "    for r in results_table:\n",
        "        print(f\"{r['Fold']:<30} | Huber Loss: {r['Best_MSE']:.4f}\")"
      ],
      "metadata": {
        "id": "BOo2lwqf2VNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration (Must match your training config) ---\n",
        "SEQ_LEN = 21\n",
        "LSTM_HIDDEN = 64\n",
        "LSTM_LAYERS = 2\n",
        "DROPOUT = 0.4\n",
        "FUT_DIM = 10   # Approx columns: Open, High, Low, Close, Vol, OI, RSI, MACD, BB, BB_width etc.\n",
        "              # Adjust this if your feature count differs (e.g. 9 or 10)\n",
        "NUM_AREAS = 6 # Based on region2id count\n",
        "\n",
        "class CornPriceModel(nn.Module):\n",
        "    def __init__(self, num_areas=NUM_AREAS, fut_dim=FUT_DIM):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Image Feature Bottleneck (ResNet 2048 -> 64)\n",
        "        self.img_bottleneck = nn.Sequential(\n",
        "            nn.Linear(2048, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # 2. Future Feature Normalization\n",
        "        # Using BatchNorm1d. Expects (N, C, L), so we permute in forward.\n",
        "        self.fut_norm = nn.BatchNorm1d(fut_dim)\n",
        "\n",
        "        # 3. Embeddings\n",
        "        self.area_embed = nn.Embedding(num_areas, 16)\n",
        "        self.time_embed = nn.Embedding(12, 8) # Month 0-11\n",
        "\n",
        "        # 4. LSTM\n",
        "        # Input = img(64) + fut(fut_dim) + area(16) + time(8)\n",
        "        self.lstm_input_dim = 64 + fut_dim + 16 + 8\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.lstm_input_dim,\n",
        "            hidden_size=LSTM_HIDDEN,\n",
        "            num_layers=LSTM_LAYERS,\n",
        "            batch_first=True,\n",
        "            dropout=DROPOUT\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(DROPOUT)\n",
        "        self.head = nn.Linear(LSTM_HIDDEN, 1)\n",
        "\n",
        "    def forward(self, img, fut, aid, time):\n",
        "        # img: (Batch, Seq, 2048)\n",
        "        # fut: (Batch, Seq, Fut_Dim)\n",
        "        # aid: (Batch,)\n",
        "        # time: (Batch,)\n",
        "\n",
        "        # Process Images\n",
        "        img_c = self.img_bottleneck(img) # -> (Batch, Seq, 64)\n",
        "\n",
        "        # Process Futures (Permute for BN1d: N, Seq, C -> N, C, Seq)\n",
        "        fut_perm = fut.permute(0, 2, 1)\n",
        "        fut_c = self.fut_norm(fut_perm)\n",
        "        fut_c = fut_c.permute(0, 2, 1)   # -> (Batch, Seq, C)\n",
        "\n",
        "        # Process Embeddings (Expand to Sequence Length)\n",
        "        area_v = self.area_embed(aid).unsqueeze(1).expand(-1, SEQ_LEN, -1)\n",
        "        time_v = self.time_embed(time).unsqueeze(1).expand(-1, SEQ_LEN, -1)\n",
        "\n",
        "        # Concatenate\n",
        "        x = torch.cat([img_c, fut_c, area_v, time_v], dim=2)\n",
        "\n",
        "        # LSTM & Head\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.head(self.dropout(out[:, -1, :]))\n",
        "\n",
        "print(\"✅ Model Class Defined\")"
      ],
      "metadata": {
        "id": "zADagHFq2V8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- User Settings ---\n",
        "MODEL_PATH = 'path.pt'  # Replace with your downloaded .pt file name\n",
        "LABEL_MEAN = 0.0532                  # REPLACE with the Mean from your training log\n",
        "LABEL_STD = 2.1245                   # REPLACE with the Std from your training log\n",
        "\n",
        "# --- Setup ---\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = CornPriceModel(num_areas=NUM_AREAS, fut_dim=FUT_DIM).to(device)\n",
        "\n",
        "# --- Load Weights ---\n",
        "try:\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "    print(f\"✅ Loaded weights from {MODEL_PATH}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ File {MODEL_PATH} not found. Initializing with random weights for demo.\")\n",
        "model.eval()\n",
        "\n",
        "# --- Create Dummy Inputs (Batch Size = 1) ---\n",
        "# Simulating: 1 sample, 21 days history\n",
        "dummy_img = torch.randn(1, SEQ_LEN, 2048).to(device)     # Random ResNet features\n",
        "dummy_fut = torch.randn(1, SEQ_LEN, FUT_DIM).to(device)  # Random Futures data\n",
        "dummy_aid = torch.tensor([0]).to(device)                 # Area ID 0\n",
        "dummy_time = torch.tensor([5]).to(device)                # Month 5 (June)\n",
        "\n",
        "# --- Simulation Context ---\n",
        "last_known_price = 450.00  # Price at day t\n",
        "real_future_price = 462.50 # The \"Real\" price at day t+14\n",
        "\n",
        "# --- Run Prediction ---\n",
        "with torch.no_grad():\n",
        "    # Model outputs normalized price difference\n",
        "    pred_norm = model(dummy_img, dummy_fut, dummy_aid, dummy_time).item()\n",
        "\n",
        "# --- De-normalize ---\n",
        "# Formula: (Output * Std) + Mean = Predicted_Diff\n",
        "pred_diff = (pred_norm * LABEL_STD) + LABEL_MEAN\n",
        "predicted_price = last_known_price + pred_diff\n",
        "\n",
        "# --- Output Report ---\n",
        "print(f\"\\n{'='*40}\")\n",
        "print(f\"🌽 CORN PRICE PREDICTION DEMO\")\n",
        "print(f\"{'='*40}\")\n",
        "print(f\"Input State\")\n",
        "print(f\"  - Sequence Length : {SEQ_LEN} days\")\n",
        "print(f\"  - Last Known Price: ${last_known_price:.2f}\")\n",
        "print(f\"----------------------------------------\")\n",
        "print(f\"Model Inference\")\n",
        "print(f\"  - Raw Output      : {pred_norm:.4f}\")\n",
        "print(f\"  - Predicted Diff  : ${pred_diff:.2f}\")\n",
        "print(f\"----------------------------------------\")\n",
        "print(f\"Results\")\n",
        "print(f\"  - PREDICTED PRICE : ${predicted_price:.2f}\")\n",
        "print(f\"  - REAL PRICE      : ${real_future_price:.2f}\")\n",
        "print(f\"  - Absolute Error  : ${abs(predicted_price - real_future_price):.2f}\")\n",
        "print(f\"{'='*40}\")"
      ],
      "metadata": {
        "id": "Jk6x6h-vwiEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Configuration ---\n",
        "AREA_NAME = 'area_51'   # The folder name\n",
        "YEAR = '2021'           # The year to filter by\n",
        "SEQ_LEN = 21            # Sequence length model expects\n",
        "\n",
        "# Path to your data (Adjust if needed!)\n",
        "IMG_BASE_DIR = '/content/drive/Shareddrives/MLSN-Team-12/MLSN-Team-12/Data/Satellite-RGB'\n",
        "FUTURES_PATH = '/content/drive/Shareddrives/MLSN-Team-12/MLSN-Team-12/Data/Futures/futures.csv'\n",
        "# ^ Check this path! It might be named differently in your drive.\n",
        "\n",
        "# --- Image Preprocessing (Standard ImageNet) ---\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def get_real_data_sequence(area_name, year, seq_len):\n",
        "    \"\"\"Loads images and futures for a specific area and year.\"\"\"\n",
        "\n",
        "    # 1. Find Images\n",
        "    area_dir = os.path.join(IMG_BASE_DIR, area_name)\n",
        "    if not os.path.exists(area_dir):\n",
        "        raise FileNotFoundError(f\"Directory not found: {area_dir}\")\n",
        "\n",
        "    all_files = sorted([f for f in os.listdir(area_dir) if f.endswith('.png')])\n",
        "\n",
        "    # Filter for the specific year\n",
        "    target_files = [f for f in all_files if year in f]\n",
        "\n",
        "    if len(target_files) < seq_len:\n",
        "        print(f\"⚠️ Warning: Only found {len(target_files)} images for {year}. Need {seq_len}.\")\n",
        "        # Fallback: take last available images regardless of year if needed, or just return what we have\n",
        "        target_files = all_files[-seq_len:]\n",
        "    else:\n",
        "        # Take the LAST sequence of the year (most recent data for that year)\n",
        "        target_files = target_files[-seq_len:]\n",
        "\n",
        "    print(f\"✅ Found {len(target_files)} images. Date range: {target_files[0]} to {target_files[-1]}\")\n",
        "\n",
        "    # 2. Process Images -> ResNet Features\n",
        "    # Ensure 'resnet' is defined and on device (from your Main.ipynb)\n",
        "    resnet.eval()\n",
        "    img_feats = []\n",
        "    dates = []\n",
        "\n",
        "    print(\"📸 Processing images through ResNet...\")\n",
        "    with torch.no_grad():\n",
        "        for fname in target_files:\n",
        "            # Parse Date\n",
        "            date_str = os.path.splitext(fname)[0].split('_')[-1]\n",
        "            dates.append(date_str)\n",
        "\n",
        "            # Load & Process\n",
        "            path = os.path.join(area_dir, fname)\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            input_tensor = preprocess(img).unsqueeze(0).to(device)\n",
        "\n",
        "            # Extract Feature\n",
        "            feat = resnet(input_tensor) # (1, 2048)\n",
        "            img_feats.append(feat)\n",
        "\n",
        "    # Stack into (1, Seq_Len, 2048)\n",
        "    img_tensor = torch.stack(img_feats, dim=1)\n",
        "\n",
        "    # 3. Load & Align Futures\n",
        "    print(\"📉 Loading Futures data...\")\n",
        "    fut_df = pd.read_csv(FUTURES_PATH)\n",
        "\n",
        "    # Ensure dates are strings for matching\n",
        "    fut_df['Date'] = fut_df['Date'].astype(str)\n",
        "\n",
        "    # Select rows matching our image dates\n",
        "    # Note: This assumes columns 1 onwards are the features (0 is Date)\n",
        "    # You might need to adjust column slicing [:, 1:] if your CSV structure is different\n",
        "    matched_data = []\n",
        "    for d in dates:\n",
        "        row = fut_df[fut_df['Date'] == d]\n",
        "        if not row.empty:\n",
        "            # Drop Date column and convert to float\n",
        "            data_row = row.iloc[0, 1:].values.astype(float)\n",
        "            matched_data.append(data_row)\n",
        "        else:\n",
        "            print(f\"⚠️ Missing futures data for {d}, filling with zeros.\")\n",
        "            matched_data.append(np.zeros(10)) # Assuming 10 dims\n",
        "\n",
        "    fut_tensor = torch.tensor(matched_data, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "    return img_tensor, fut_tensor, dates\n",
        "\n",
        "print(\"✅ Data Loader Ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiSL2kR1qJw9",
        "outputId": "e35d2b98-b988-4d13-91b3-9f327cb04ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Data Loader Ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# ============================================================\n",
        "# 1. CONFIGURATION\n",
        "# ============================================================\n",
        "BASE_PATH = '/content/drive/Shareddrives/MLSN-Team-12/MLSN-Team-12'\n",
        "MODEL_PATH = os.path.join(BASE_PATH, 'Models/best_model_Fold_1.pt')\n",
        "SCALAR_PATH = os.path.join(BASE_PATH, 'Models/scalar_Fold_1.pkl')\n",
        "WEATHER_DIR = os.path.join(BASE_PATH, 'Data/Weather_Base')\n",
        "FUTURES_PATH = os.path.join(BASE_PATH, 'Data/Futures/futures_data.csv')\n",
        "AREA_FEATURE_PATH = os.path.join(BASE_PATH, 'Data/Satellite-RGB/area_16/features.csv')\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "SEQ_LEN = 21\n",
        "NUM_AREAS = 190\n",
        "\n",
        "# ============================================================\n",
        "# 2. MODEL ARCHITECTURE\n",
        "# ============================================================\n",
        "class CornETFModelFinal(nn.Module):\n",
        "    def __init__(self, img_dim, fut_dim, num_areas):\n",
        "        super().__init__()\n",
        "        self.img_bottleneck = nn.Sequential(\n",
        "            nn.Linear(img_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        self.fut_norm = nn.LayerNorm(fut_dim)\n",
        "        self.area_embed = nn.Embedding(num_areas, 16)\n",
        "        self.time_embed = nn.Linear(1, 8)\n",
        "\n",
        "        self.lstm_input_dim = 64 + fut_dim + 16 + 8\n",
        "        self.lstm = nn.LSTM(input_size=self.lstm_input_dim, hidden_size=64, num_layers=2, batch_first=True, dropout=0.4)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.head = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, img, fut, aid, time):\n",
        "        img_c = self.img_bottleneck(img)\n",
        "        fut_c = self.fut_norm(fut)\n",
        "        area_v = self.area_embed(aid)\n",
        "        time_v = self.time_embed(time)\n",
        "        x = torch.cat([img_c, fut_c, area_v, time_v], dim=2)\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.head(self.dropout(out[:, -1, :]))\n",
        "\n",
        "# ============================================================\n",
        "# 3. FEATURE ENGINEERING\n",
        "# ============================================================\n",
        "def engineer_futures(df):\n",
        "    df = df.copy()\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "    # Seasonality\n",
        "    day = df['Date'].dt.dayofyear\n",
        "    df['Day_Sin'] = np.sin(2 * np.pi * day / 365.0)\n",
        "    df['Day_Cos'] = np.cos(2 * np.pi * day / 365.0)\n",
        "\n",
        "    # Missing Columns check\n",
        "    if 'Close Percent Change' in df.columns:\n",
        "        df = df.rename(columns={'Close Percent Change': 'Close_pct_change'})\n",
        "    if 'Close_pct_change' not in df.columns:\n",
        "        df['Close_pct_change'] = df['Close'].pct_change() * 100.0\n",
        "    if 'Volume' in df.columns:\n",
        "        df['Volume_log'] = np.log1p(df['Volume'])\n",
        "    else:\n",
        "        df['Volume_log'] = 0.0\n",
        "\n",
        "    # Technicals\n",
        "    delta = df['Close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "    rs = gain / loss\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "    df['RSI'] = df['RSI'].fillna(50)\n",
        "\n",
        "    ema12 = df['Close'].ewm(span=12, adjust=False).mean()\n",
        "    ema26 = df['Close'].ewm(span=26, adjust=False).mean()\n",
        "    df['MACD'] = ema12 - ema26\n",
        "\n",
        "    sma20 = df['Close'].rolling(window=20).mean()\n",
        "    std20 = df['Close'].rolling(window=20).std()\n",
        "    bb_upper = sma20 + (std20 * 2)\n",
        "    bb_lower = sma20 - (std20 * 2)\n",
        "    df['BB_PctB'] = (df['Close'] - bb_lower) / (bb_upper - bb_lower)\n",
        "    df['BB_PctB'] = df['BB_PctB'].fillna(0.5)\n",
        "\n",
        "    return df.dropna().reset_index(drop=True)\n",
        "\n",
        "def get_area_weather(weather_dir, area_name):\n",
        "    target_file = None\n",
        "    for f in os.listdir(weather_dir):\n",
        "        if area_name in f and f.endswith('.csv'):\n",
        "            target_file = os.path.join(weather_dir, f)\n",
        "            break\n",
        "    if not target_file: return None\n",
        "    df = pd.read_csv(target_file)\n",
        "    date_col = next((c for c in df.columns if 'date' in c.lower()), None)\n",
        "    if not date_col: return None\n",
        "    df = df.rename(columns={date_col: 'Date'})\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df = df.sort_values('Date')\n",
        "    try:\n",
        "        t_mean = (df['temp_max_C'] + df['temp_min_C']) / 2\n",
        "        df['GDD'] = (t_mean - 10).clip(lower=0)\n",
        "        df['GDD_Cum_7d'] = df['GDD'].rolling(7).sum()\n",
        "        df['Precip_Sum_7d'] = df['precip_mm'].rolling(7).sum()\n",
        "        return df[['Date', 'GDD_Cum_7d', 'Precip_Sum_7d']].copy()\n",
        "    except: return None\n",
        "\n",
        "# ============================================================\n",
        "# 4. EXECUTION PIPELINE\n",
        "# ============================================================\n",
        "print(\"🔄 Loading & Engineering Data...\")\n",
        "\n",
        "if os.path.exists(SCALAR_PATH):\n",
        "    with open(SCALAR_PATH, 'rb') as f:\n",
        "        scalars = pickle.load(f)\n",
        "    if isinstance(scalars, dict):\n",
        "        LABEL_MEAN, LABEL_STD = scalars.get('mean', 0), scalars.get('std', 1)\n",
        "    else:\n",
        "        LABEL_MEAN, LABEL_STD = scalars[0], scalars[1]\n",
        "else:\n",
        "    LABEL_MEAN, LABEL_STD = 0.0, 1.0\n",
        "\n",
        "# 1. Load Satellite (The \"Anchor\" Dataset)\n",
        "sat_df = pd.read_csv(AREA_FEATURE_PATH)\n",
        "sat_df['Date'] = pd.to_datetime(sat_df['Date'])\n",
        "\n",
        "# 2. Load Weather\n",
        "weather_df = get_area_weather(WEATHER_DIR, 'area_16')\n",
        "\n",
        "# 3. Load & Engineer Futures\n",
        "fut_raw = pd.read_csv(FUTURES_PATH)\n",
        "fut_df = engineer_futures(fut_raw)\n",
        "\n",
        "# 4. MERGE (LEFT JOIN FIX)\n",
        "# We use left join on sat_df to keep ALL 29 images, even if they fall on weekends.\n",
        "if weather_df is not None:\n",
        "    merged = pd.merge(sat_df, weather_df, on='Date', how='left')\n",
        "    merged[['GDD_Cum_7d', 'Precip_Sum_7d']] = merged[['GDD_Cum_7d', 'Precip_Sum_7d']].ffill().bfill().fillna(0)\n",
        "else:\n",
        "    merged = sat_df\n",
        "\n",
        "# Merge with Futures\n",
        "final_df = pd.merge(merged, fut_df, on='Date', how='left')\n",
        "\n",
        "# CRITICAL FIX: Forward fill weekend gaps for market data\n",
        "final_df = final_df.sort_values('Date').ffill().bfill()\n",
        "\n",
        "# Filter for 2021\n",
        "final_df = final_df[final_df['Date'].dt.year == 2018]\n",
        "\n",
        "if len(final_df) < SEQ_LEN: raise ValueError(f\"Not enough data. Found {len(final_df)} rows.\")\n",
        "\n",
        "# Select the LAST 21 images (should now be a partial year, not the whole thing)\n",
        "seq_df = final_df.iloc[:SEQ_LEN].copy()\n",
        "\n",
        "# Prepare Tensors\n",
        "img_cols = [c for c in final_df.columns if c.startswith('feat_')]\n",
        "exclude = ['Date', 'Area', 'Time Delta'] + img_cols\n",
        "fut_cols = [c for c in final_df.columns if c not in exclude]\n",
        "\n",
        "t_img = torch.tensor(seq_df[img_cols].values, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
        "t_fut = torch.tensor(seq_df[fut_cols].values, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
        "t_aid = torch.LongTensor([0] * SEQ_LEN).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "dates = seq_df['Date'].values\n",
        "dts = np.diff(dates.astype('datetime64[D]').astype(int))\n",
        "dts = np.insert(dts, 0, 0).astype(np.float32)\n",
        "t_time = torch.tensor(dts).reshape(1, SEQ_LEN, 1).to(DEVICE)\n",
        "\n",
        "# ============================================================\n",
        "# 5. RUN & REPORT\n",
        "# ============================================================\n",
        "print(\"🏗️  Running Model...\")\n",
        "model = CornETFModelFinal(len(img_cols), len(fut_cols), NUM_AREAS).to(DEVICE)\n",
        "\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "else:\n",
        "    print(\"❌ Model weights not found!\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred_norm = model(t_img, t_fut, t_aid, t_time).item()\n",
        "\n",
        "pred_diff = (pred_norm * LABEL_STD) + LABEL_MEAN\n",
        "\n",
        "# --- ACCURACY CHECK ---\n",
        "last_date = seq_df['Date'].iloc[-1]\n",
        "target_date = last_date + pd.Timedelta(days=14)\n",
        "\n",
        "# Find 'Real' price ~14 days later in the full futures data\n",
        "# We look in fut_df (daily market data) not final_df (sparse satellite data)\n",
        "future_rows = fut_df[fut_df['Date'] >= target_date]\n",
        "\n",
        "close_col = next((c for c in fut_cols if 'close' in c.lower() and 'pct' not in c.lower()), None)\n",
        "last_price = seq_df[close_col].iloc[-1] if close_col else 0.0\n",
        "\n",
        "if not future_rows.empty:\n",
        "    real_future_price = future_rows.iloc[0]['Close']\n",
        "    real_actual_date = future_rows.iloc[0]['Date'].strftime('%Y-%m-%d')\n",
        "    real_change = real_future_price - last_price\n",
        "    accuracy_msg = f\"${real_change:+.2f}\"\n",
        "    error_val = abs(real_change - pred_diff)\n",
        "else:\n",
        "    real_change = 0.0\n",
        "    accuracy_msg = \"N/A\"\n",
        "    error_val = 0.0\n",
        "\n",
        "\n",
        "print(f\"PREDICTED PRICE   : ${last_price + pred_diff:.2f}\")\n",
        "print(f\"REAL PRICE        : ${last_price + real_change:.2f}\")\n",
        "print(f\"{'='*45}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8zBQWgeqiyM",
        "outputId": "e2211ae6-1628-42c9-bee9-cc45d1885688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Loading & Engineering Data...\n",
            "🏗️  Running Model...\n",
            "PREDICTED PRICE   : $370.87\n",
            "REAL PRICE        : $374.00\n",
            "=============================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# ============================================================\n",
        "# 1. CONFIGURATION\n",
        "# ============================================================\n",
        "BASE_PATH = '/content/drive/Shareddrives/MLSN-Team-12/MLSN-Team-12'\n",
        "MODEL_PATH = os.path.join(BASE_PATH, 'Models/best_model_Fold_1.pt')\n",
        "SCALER_PATH = os.path.join(BASE_PATH, 'Models/scaler_Fold_1.pkl')\n",
        "\n",
        "WEATHER_DIR = os.path.join(BASE_PATH, 'Data/Weather_Base')\n",
        "FUTURES_PATH = os.path.join(BASE_PATH, 'Data/Futures/futures_data.csv')\n",
        "AREA_FEATURE_PATH = os.path.join(BASE_PATH, 'Data/Satellite-RGB/area_16/features.csv')\n",
        "AREA_ID_INT = 15  # Area 16\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "SEQ_LEN = 21\n",
        "NUM_AREAS = 190\n",
        "EXPECTED_FUT_DIM = 15 # The model was trained on 15 features. We MUST provide 15.\n",
        "\n",
        "# ============================================================\n",
        "# 2. MODEL ARCHITECTURE\n",
        "# ============================================================\n",
        "class CornETFModelFinal(nn.Module):\n",
        "    def __init__(self, img_dim, fut_dim, num_areas):\n",
        "        super().__init__()\n",
        "        self.img_bottleneck = nn.Sequential(\n",
        "            nn.Linear(img_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        self.fut_norm = nn.LayerNorm(fut_dim)\n",
        "        self.area_embed = nn.Embedding(num_areas, 16)\n",
        "        self.time_embed = nn.Linear(1, 8)\n",
        "\n",
        "        self.lstm_input_dim = 64 + fut_dim + 16 + 8\n",
        "        self.lstm = nn.LSTM(input_size=self.lstm_input_dim, hidden_size=64, num_layers=2, batch_first=True, dropout=0.4)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.head = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, img, fut, aid, time):\n",
        "        img_c = self.img_bottleneck(img)\n",
        "        fut_c = self.fut_norm(fut)\n",
        "        area_v = self.area_embed(aid)\n",
        "        time_v = self.time_embed(time)\n",
        "        x = torch.cat([img_c, fut_c, area_v, time_v], dim=2)\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.head(self.dropout(out[:, -1, :]))\n",
        "\n",
        "# ============================================================\n",
        "# 3. FEATURE ENGINEERING\n",
        "# ============================================================\n",
        "def engineer_futures(df):\n",
        "    df = df.copy()\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "    day = df['Date'].dt.dayofyear\n",
        "    df['Day_Sin'] = np.sin(2 * np.pi * day / 365.0)\n",
        "    df['Day_Cos'] = np.cos(2 * np.pi * day / 365.0)\n",
        "\n",
        "    if 'Close Percent Change' in df.columns:\n",
        "        df = df.rename(columns={'Close Percent Change': 'Close_pct_change'})\n",
        "    if 'Close_pct_change' not in df.columns:\n",
        "        df['Close_pct_change'] = df['Close'].pct_change() * 100.0\n",
        "    if 'Volume' in df.columns:\n",
        "        df['Volume_log'] = np.log1p(df['Volume'])\n",
        "    else:\n",
        "        df['Volume_log'] = 0.0\n",
        "\n",
        "    delta = df['Close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "    rs = gain / loss\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "    df['RSI'] = df['RSI'].fillna(50)\n",
        "\n",
        "    ema12 = df['Close'].ewm(span=12, adjust=False).mean()\n",
        "    ema26 = df['Close'].ewm(span=26, adjust=False).mean()\n",
        "    df['MACD'] = ema12 - ema26\n",
        "\n",
        "    sma20 = df['Close'].rolling(window=20).mean()\n",
        "    std20 = df['Close'].rolling(window=20).std()\n",
        "    bb_upper = sma20 + (std20 * 2)\n",
        "    bb_lower = sma20 - (std20 * 2)\n",
        "    df['BB_PctB'] = (df['Close'] - bb_lower) / (bb_upper - bb_lower)\n",
        "    df['BB_PctB'] = df['BB_PctB'].fillna(0.5)\n",
        "\n",
        "    return df.dropna().reset_index(drop=True)\n",
        "\n",
        "def get_area_weather(weather_dir, area_name):\n",
        "    target_file = None\n",
        "    for f in os.listdir(weather_dir):\n",
        "        if area_name in f and f.endswith('.csv'):\n",
        "            target_file = os.path.join(weather_dir, f)\n",
        "            break\n",
        "    if not target_file: return None\n",
        "    df = pd.read_csv(target_file)\n",
        "    date_col = next((c for c in df.columns if 'date' in c.lower()), None)\n",
        "    if not date_col: return None\n",
        "    df = df.rename(columns={date_col: 'Date'})\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df = df.sort_values('Date')\n",
        "    try:\n",
        "        t_mean = (df['temp_max_C'] + df['temp_min_C']) / 2\n",
        "        df['GDD'] = (t_mean - 10).clip(lower=0)\n",
        "        df['GDD_Cum_7d'] = df['GDD'].rolling(7).sum()\n",
        "        df['Precip_Sum_7d'] = df['precip_mm'].rolling(7).sum()\n",
        "        return df[['Date', 'GDD_Cum_7d', 'Precip_Sum_7d']].copy()\n",
        "    except: return None\n",
        "\n",
        "# ============================================================\n",
        "# 4. PREDICTION FUNCTION\n",
        "# ============================================================\n",
        "def predict_scenario(target_date_str, description, model, feature_scaler, full_df):\n",
        "    target_date = pd.Timestamp(target_date_str)\n",
        "    cutoff_date = target_date - pd.Timedelta(days=14)\n",
        "\n",
        "    # Filter History\n",
        "    history = full_df[full_df['Date'] <= cutoff_date].copy()\n",
        "\n",
        "    if len(history) < SEQ_LEN:\n",
        "        print(f\"⚠️ SKIPPING {target_date_str}: Not enough history found.\")\n",
        "        return\n",
        "\n",
        "    # Grab Sequence\n",
        "    seq_df = history.tail(SEQ_LEN).copy()\n",
        "\n",
        "    # CHECK DATA FRESHNESS\n",
        "    last_img_date = seq_df['Date'].iloc[-1]\n",
        "    days_stale = (cutoff_date - last_img_date).days\n",
        "    freshness_status = \"✅ Fresh\" if days_stale < 10 else f\"⚠️ Stale ({days_stale} days old)\"\n",
        "\n",
        "    # Prepare Tensors\n",
        "    img_cols = [c for c in full_df.columns if c.startswith('feat_')]\n",
        "    exclude = ['Date', 'Area', 'Time Delta'] + img_cols\n",
        "    fut_cols = [c for c in full_df.columns if c not in exclude]\n",
        "\n",
        "    # --- SAFETY PADDING (THE FIX) ---\n",
        "    # Model expects 15 features. If we have 13, we pad with zeros.\n",
        "    raw_fut = seq_df[fut_cols].values\n",
        "    current_dim = raw_fut.shape[1]\n",
        "\n",
        "    if current_dim < EXPECTED_FUT_DIM:\n",
        "        diff = EXPECTED_FUT_DIM - current_dim\n",
        "        # Create zeros of shape (21, diff)\n",
        "        zeros = np.zeros((raw_fut.shape[0], diff))\n",
        "        # Stack them\n",
        "        raw_fut = np.hstack([raw_fut, zeros])\n",
        "    # --------------------------------\n",
        "\n",
        "    # Scale Features\n",
        "    if feature_scaler:\n",
        "        scaled_fut = feature_scaler.transform(raw_fut)\n",
        "    else:\n",
        "        scaled_fut = raw_fut\n",
        "\n",
        "    t_img = torch.tensor(seq_df[img_cols].values, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
        "    t_fut = torch.tensor(scaled_fut, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
        "    t_aid = torch.LongTensor([AREA_ID_INT] * SEQ_LEN).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    dates = seq_df['Date'].values\n",
        "    dts = np.diff(dates.astype('datetime64[D]').astype(int))\n",
        "    dts = np.insert(dts, 0, 0).astype(np.float32)\n",
        "    t_time = torch.tensor(dts).reshape(1, SEQ_LEN, 1).to(DEVICE)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        pred_raw = model(t_img, t_fut, t_aid, t_time).item()\n",
        "\n",
        "    # Fold 1 Stats\n",
        "    LABEL_MEAN, LABEL_STD = 0.8693, 4.9082\n",
        "    pred_pct = (pred_raw * LABEL_STD) + LABEL_MEAN\n",
        "\n",
        "    # Check Actuals\n",
        "    last_price = seq_df['Close'].iloc[-1]\n",
        "    pred_price = last_price * (1 + pred_pct / 100.0)\n",
        "\n",
        "    # Find real price\n",
        "    future_data = fut_df[fut_df['Date'] >= target_date]\n",
        "\n",
        "    print(f\"\\n📝 SCENARIO: {description}\")\n",
        "    print(f\"   Target Date: {target_date.date()} | Last Image Seen: {last_img_date.date()} ({freshness_status})\")\n",
        "    print(f\"   AI Prediction: {pred_pct:+.2f}%  (${pred_price:.2f})\")\n",
        "\n",
        "    if not future_data.empty:\n",
        "        real_price = future_data.iloc[0]['Close']\n",
        "        real_pct = ((real_price - last_price) / last_price) * 100.0\n",
        "        diff = abs(real_pct - pred_pct)\n",
        "\n",
        "        # Grading\n",
        "        grade = \"✅ WIN\" if diff < 2.0 else \"❌ MISS\"\n",
        "        if \"Stale\" in freshness_status and grade == \"❌ MISS\": grade += \" (Due to Latency)\"\n",
        "\n",
        "        print(f\"   Actual Market: {real_pct:+.2f}%  (${real_price:.2f})\")\n",
        "        print(f\"   Result: {grade} (Diff: {diff:.2f}%)\")\n",
        "    else:\n",
        "        print(\"   Actual Market: Future data not available.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# 5. EXECUTION\n",
        "# ============================================================\n",
        "print(\"🔄 Initializing System...\")\n",
        "\n",
        "# Load Scaler\n",
        "if os.path.exists(SCALER_PATH):\n",
        "    with open(SCALER_PATH, 'rb') as f:\n",
        "        feature_scaler = pickle.load(f)\n",
        "else:\n",
        "    feature_scaler = None\n",
        "    print(\"⚠️ No Scaler Found!\")\n",
        "\n",
        "# Load Data Frames\n",
        "sat_df = pd.read_csv(AREA_FEATURE_PATH)\n",
        "sat_df['Date'] = pd.to_datetime(sat_df['Date'])\n",
        "weather_df = get_area_weather(WEATHER_DIR, 'area_16')\n",
        "fut_raw = pd.read_csv(FUTURES_PATH)\n",
        "fut_df = engineer_futures(fut_raw)\n",
        "\n",
        "# Merge\n",
        "if weather_df is not None:\n",
        "    merged = pd.merge(sat_df, weather_df, on='Date', how='left')\n",
        "    merged[['GDD_Cum_7d', 'Precip_Sum_7d']] = merged[['GDD_Cum_7d', 'Precip_Sum_7d']].ffill().bfill().fillna(0)\n",
        "else:\n",
        "    print(\"⚠️ WARNING: Weather data not found for this area. Padding missing columns.\")\n",
        "    merged = sat_df\n",
        "\n",
        "full_df = pd.merge(merged, fut_df, on='Date', how='left')\n",
        "full_df = full_df.sort_values('Date').ffill().bfill()\n",
        "\n",
        "# Prepare Model\n",
        "img_cols = [c for c in full_df.columns if c.startswith('feat_')]\n",
        "# fut_cols logic is dynamic inside the predict function, but we MUST init model with 15\n",
        "model = CornETFModelFinal(len(img_cols), EXPECTED_FUT_DIM, NUM_AREAS).to(DEVICE)\n",
        "\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    model.eval()\n",
        "    print(\"✅ Model Loaded.\")\n",
        "else:\n",
        "    print(\"❌ Model File Missing!\")\n",
        "\n",
        "# ============================================================\n",
        "# 6. RUN THE TEST SCENARIOS\n",
        "# ============================================================\n",
        "scenarios = [\n",
        "    # 2021: The Validation Year (Fold 1)\n",
        "    (\"2021-05-07\", \"The 2021 Rally Peak\"),\n",
        "    (\"2021-07-01\", \"The Summer Correction\"),\n",
        "\n",
        "    # 2022: The Out-of-Sample Year (Fold 2)\n",
        "    (\"2022-03-01\", \"The Ukraine War Shock\"),\n",
        "    (\"2022-06-15\", \"The 2022 Highs\"),\n",
        "\n",
        "    # 2023: The Crash (Fold 3)\n",
        "    (\"2023-01-15\", \"The 2023 Decline\")\n",
        "]\n",
        "\n",
        "print(\"\\n🚀 STARTING MULTI-DATE SIMULATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for date, desc in scenarios:\n",
        "    predict_scenario(date, desc, model, feature_scaler, full_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChSxCBiNyaqy",
        "outputId": "16d4d16a-faa0-40c4-bb8b-586d7c438e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Initializing System...\n",
            "✅ Model Loaded.\n",
            "\n",
            "🚀 STARTING MULTI-DATE SIMULATION\n",
            "============================================================\n",
            "\n",
            "📝 SCENARIO: The 2021 Rally Peak\n",
            "   Target Date: 2021-05-07 | Last Image Seen: 2021-04-15 (✅ Fresh)\n",
            "   AI Prediction: +0.80%  ($594.74)\n",
            "   Actual Market: +30.97%  ($772.75)\n",
            "   Result: ❌ MISS (Diff: 30.17%)\n",
            "------------------------------------------------------------\n",
            "\n",
            "📝 SCENARIO: The Summer Correction\n",
            "   Target Date: 2021-07-01 | Last Image Seen: 2021-06-17 (✅ Fresh)\n",
            "   AI Prediction: +0.76%  ($637.80)\n",
            "   Actual Market: +13.70%  ($719.75)\n",
            "   Result: ❌ MISS (Diff: 12.95%)\n",
            "------------------------------------------------------------\n",
            "\n",
            "📝 SCENARIO: The Ukraine War Shock\n",
            "   Target Date: 2022-03-01 | Last Image Seen: 2022-02-14 (✅ Fresh)\n",
            "   AI Prediction: +0.51%  ($659.13)\n",
            "   Actual Market: +12.81%  ($739.75)\n",
            "   Result: ❌ MISS (Diff: 12.29%)\n",
            "------------------------------------------------------------\n",
            "\n",
            "📝 SCENARIO: The 2022 Highs\n",
            "   Target Date: 2022-06-15 | Last Image Seen: 2022-05-23 (✅ Fresh)\n",
            "   AI Prediction: +0.42%  ($789.59)\n",
            "   Actual Market: -1.56%  ($774.00)\n",
            "   Result: ✅ WIN (Diff: 1.98%)\n",
            "------------------------------------------------------------\n",
            "\n",
            "📝 SCENARIO: The 2023 Decline\n",
            "   Target Date: 2023-01-15 | Last Image Seen: 2022-12-24 (✅ Fresh)\n",
            "   AI Prediction: +1.01%  ($663.90)\n",
            "   Actual Market: +4.26%  ($685.25)\n",
            "   Result: ❌ MISS (Diff: 3.25%)\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}